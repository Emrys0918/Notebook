# 1. 研究现状速览

近两年，大模型（LLM/VLM/VLA）在具身智能的**高层任务规划→中层子目标/约束→低层控制**链路上快速融合，形成三条主要路线：
- **自然语言→规划（Natural Language→Plan）**：把自然语言转成形式化目标（PDDL/LTL/行为树）并调用经典规划器或验证器。代表：SayCan（语言+可供性/价值函数,每个技能都有对应的value function，能够估计在当前环境下运行成功的概率，反映了执行此技能之后是否会真正产生进度，离目标更近）、LLM→PDDL/LTL、场景图+LTL的层级规划等。  
- **语言-视觉-动作端到端（VLA）**：以大规模操控数据预训练策略/条件策略，语言作为条件信号指导动作生成。代表：RT-2、OpenVLA、Octo、ACT、Diffusion Policy。  
- **推理式智能体（Reasoning Agents）**：通过 ReAct/Reflexion/ToT/LtM 等“**分解—搜索—自反思**”范式，把抽象问题分成子问题并与环境交互校正。

这些方向的共同趋势：**形式化与验证回归（LTL/PDDL/模型检查）**、**检索与记忆（RAG/策略库）**、**世界模型一致性**、**跨模态的可供性/约束落地**。综述类文章也开始系统梳理 LLM×机器人/语言落地/优化-TAMP 的版图。

---

# 2. 技术路线与研究进展

## 2.1 语言→形式化→可验证规划
- **LLM→LTL/PDDL/行为树**：用 LLM 将任务翻译为 LTL/PDDL，再由经典规划器求解或由验证器约束生成；近年来出现**层级 LTL**、**同义等价投票**、**交互式模型精化**与**环境交互校验**等稳健化手段。  
- **场景图+LTL 的层级规划**：将环境抽象为**功能/语义场景图**，由 LLM 生成 LTL 自动机并在图上做最优层级规划，兼顾语义与可达性。  
- **LLM→PDDL 的系统化研究**：包括自动化提示生成、从 NL 到 PDDL 的“翻译-推理-编译（TIC）”、以及把 LLM 作为**领域/问题生成器与形式化器**，并与外部规划器/验证器闭环。

**要点**：形式化的优势在于**可解释、可验证、可组合**，但生成质量与环境几何/动力学一致性仍是瓶颈，需借助**外部求解器/验证器**和**交互式修复**。

## 2.2 端到端 VLA / 通用策略
- **RT-2**：把互联网视觉-语言知识迁移到机器人控制，提升泛化与语义推理能力。  
- **OpenVLA**：开源 7B VLA，基于 Open-X Embodiment 的 97 万条演示，支持快速参数高效微调。  
- **Octo**：在 80 万条多机器人多视角轨迹上预训练的通用变换器策略，可语言或目标图像条件化。  
- **动作建模**：**ACT** 用动作分块+Transformer，**Diffusion Policy** 用扩散过程建模多峰动作分布，鲁棒稳定。

**要点**：端到端策略在低层控制强，但对**长地平线逻辑约束/可供性**的显式满足较弱，常与上层**文本-形式化规划**或**检索记忆**组合。

## 2.3 推理式智能体与自我纠错
- **ReAct/Reflexion/ToT/LtM**：通过“**思维链→分支搜索/回溯→自我反思**”增强规划与容错；适合作为**语言-形式化生成**与**动作序列**的上层控制逻辑。  
- **检索增强与策略记忆**：将外部**策略库/经验库**纳入（RAEA、P-RAG 等），以情境相似性检索过往策略/轨迹，提升泛化与数据效率。  
- **形式验证结合**：把 LTL/模型检查嵌入智能体闭环中，实现**不安全动作剪枝、违规解释与自动修复（repair）**。

---

# 3. 评测维度与常见瓶颈

- **跨层一致性**：NL→形式化→子目标→动作的链路一致性最易出错（模板歧义、前置条件遗漏、负面约束忽视）。形式化投票/验证能缓解但仍依赖高质量感知与对象状态估计。  
- **可供性与安全性**：语言模型生成的步骤易忽视可达性/几何/动力学限制，需要**价值函数/可供性模型/轨迹优化器**介入。  
- **长地平线与泛化**：端到端策略对超长任务、域转移、稀疏监督仍敏感；检索记忆与中间里程碑（视觉或符号）有帮助。  
- **形式化生成质量**：LLM 生成 PDDL/LTL 易有语法/语义错误；自动化提示、自举式精化与交互校验成为必需。

---


# 4. 可研究方向   【【【【【【【这个你可以结合你想做的方向 在这说感觉比较合适  


1) **“选择-投影-生成”的模块化软-RAG 规划器  
   在 **NL→LTL/PDDL/场景图约束**与**子目标分解**两层引入**选择器**做软检索（对象/属性/前置条件/LTL 模板），**投影器**把检索子集结构化注入上下文，再由 LLM 产出形式化规格或子目标序列。  
   结合**同义等价投票/一致性检查**与**外部规划器**，减少幻觉与遗漏。

2) **“规划-即-验证”：LTL/模型检查驱动的动作序列修复**  
   对候选动作序列做**可满足性检查**与**最小改错**（repair），把失败案例转成**反事实指令**用于 SFT/LoRA 精调，形成**错误→修复提示模板库**。

3) **世界模型一致性回路（Transition-Consistency Loop）**  
   训练/蒸馏轻量**符号-感知混合**转移模型，对计划做 1–k 步 roll-out；若预测与计划不一致，触发**自反思/再检索/再验证**。与端到端策略（OpenVLA/Octo/DP/ACT）解耦对接。

4) **检索增强的策略/程序记忆（Policy-RAG）**  
   维护**策略片段/执行轨迹/失败-修复对**的记忆库；按目标与场景图相似性检索“可复用子程序/动作宏”，再由 LLM 进行**拼接+验证**。

5) **“场景图×功能图”与可供性学习**  
   以**功能/关系增强的 3D 场景图**（功能边/可供性标签）为中介，统一“看得懂→能计划→能执行”；配套**LLM-to-LTL on scene graph**与**可供性约束模板库**。

6) **多范式协同：端到端策略 + 形式化监督信号**  
   以 LTL/PDDL 生成“**弱标签/合规分**”监督端到端策略（如 DP/ACT/OpenVLA/Octo）的中间层或采样器，做**偏好/拒绝**信号，提升长程任务稳定性与合规性。
