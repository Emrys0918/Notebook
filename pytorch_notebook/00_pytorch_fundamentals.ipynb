{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc5e0e4a",
   "metadata": {},
   "source": [
    "# Pytorch基础"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54754711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欢迎来到Pytorch的学习\n"
     ]
    }
   ],
   "source": [
    "print(\"欢迎来到Pytorch的学习\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658a5c3f",
   "metadata": {},
   "source": [
    "## 导入需要使用的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36d90eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch_version: 2.9.0+cu128\n",
      "pandas_version: 2.3.3\n",
      "numpy_version: 2.3.4\n"
     ]
    }
   ],
   "source": [
    "# 环境准备\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "print(\"torch_version: \" + torch.__version__)\n",
    "print(\"pandas_version: \" + pd.__version__)\n",
    "print(\"numpy_version: \" + np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba51c2d",
   "metadata": {},
   "source": [
    "## 1. Tensor（张量）用来表示数据，特别是多维数值数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fb0b87",
   "metadata": {},
   "source": [
    "### 创建张量————scalar（标量）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0597bb99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scalar\n",
    "scalar = torch.tensor(7)  # 创建标量\n",
    "scalar  # 获取标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72cb774b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar.ndim # 获取标量的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8be4d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar.item() # 将标量转换为Python数值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac3b3fb",
   "metadata": {},
   "source": [
    "### 创建张量————vector（向量）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e22dd3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 7])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector\n",
    "vector = torch.tensor([7,7])\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7528a8",
   "metadata": {},
   "source": [
    "### 维度可以通过闭合方括号的数量来确定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc995885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.ndim  # 获取向量的维度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc59bcb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.shape  # 获取向量的形状（元素的个数）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9a725a",
   "metadata": {},
   "source": [
    "### 创建张量————MATRIX（矩阵）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25a89609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MARTIX\n",
    "MARTIX = torch.tensor([[1,2,3],[4,5,6]])\n",
    "MARTIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3b58604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MARTIX.ndim  # 获取矩阵的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0dca7223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MARTIX.shape # 获取矩阵的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a677514a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MARTIX[0]  # 获取矩阵的第一行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d538cf1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 5, 6])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MARTIX[1]  # 获取矩阵的第二行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc644071",
   "metadata": {},
   "source": [
    "### 创建张量————TENSOR（张量）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62750b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3],\n",
       "         [3, 6, 9],\n",
       "         [2, 4, 5]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TENSOR\n",
    "TENSOR = torch.tensor([[[1,2,3],\n",
    "                       [3,6,9],\n",
    "                       [2,4,5]]])\n",
    "TENSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "515b10f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TENSOR.ndim  # 获取张量的维度,可以更多，n维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c97612b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TENSOR.shape  # 获取张量的形状"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d858ef",
   "metadata": {},
   "source": [
    "torch.Size([3, 1, 3]) 可解读为一个3行3列的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "873dc02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [3, 6, 9],\n",
       "        [2, 4, 5]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TENSOR[0]   # 获取张量的第一个矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9864ebeb",
   "metadata": {},
   "source": [
    "Alright, it outputs `torch.Size([1, 3, 3])`.\n",
    "\n",
    "维度从外到内,这意味着有一个 3 x 3 的维度。\n",
    "\n",
    "![example of different tensor dimensions](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00-pytorch-different-tensor-dimensions.png)\n",
    "\n",
    "> **Note:** 使用小写字母表示 `scalar` and `vector` and 大写字母表示 `MATRIX` and `TENSOR`. 这是有意的. 在实践中，你经常会看到标量和向量用小写字母表示，例如 `y` or `a`. 而矩阵和张量则用大写字母表示，例如 `X` or `W`.\n",
    "\n",
    "> You also might notice the names martrix（矩阵）and tensor（张量）used interchangably（互换地）. This is common. Since in PyTorch you're often dealing with `torch.Tensor`s (hence the tensor name), however, the shape and dimensions of what's inside will dictate what it actually is.（形状和维度才是区分他们的到底是那一类的关键）\n",
    "\n",
    "Let's summarise.\n",
    "\n",
    "| Name | What is it? | Number of dimensions | Lower or upper (usually/example) |\n",
    "| ----- | ----- | ----- | ----- |\n",
    "| **scalar** | a single number | 0 | Lower (`a`) | \n",
    "| **vector** | a number with direction (e.g. wind speed with direction) but can also have many other numbers | 1 | Lower (`y`) |\n",
    "| **matrix** | a 2-dimensional array of numbers | 2 | Upper (`Q`) |\n",
    "| **tensor** | an n-dimensional array of numbers | can be any number, a 0-dimension tensor is a scalar, a 1-dimension tensor is a vector | Upper (`X`) | \n",
    "\n",
    "![scalar vector matrix tensor and what they look like](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00-scalar-vector-matrix-tensor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be74871f",
   "metadata": {},
   "source": [
    "## 随机张量\n",
    "\n",
    "We've established tensors represent some form of data.\n",
    "\n",
    "And machine learning models such as neural networks manipulate（操作） and seek patterns（模式） within tensors.\n",
    "\n",
    "But when building machine learning models with PyTorch, it's rare you'll create tensors by hand (like what we've been doing).\n",
    "\n",
    "Instead, a machine learning model often starts out with large random tensors of numbers and adjusts these random numbers as it works through data to better represent it.\n",
    "\n",
    "In essence:\n",
    "\n",
    "`Start with random numbers -> look at data -> update random numbers -> look at data -> update random numbers...`\n",
    "\n",
    "As a data scientist, you can define how the machine learning model starts (initialization), looks at data (representation) and updates (optimization) its random numbers.\n",
    "\n",
    "We'll get hands on with these steps later on.\n",
    "\n",
    "For now, let's see how to create a tensor of random numbers.\n",
    "\n",
    "We can do so using [`torch.rand()`](https://pytorch.org/docs/stable/generated/torch.rand.html) and passing in the `size` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13f07067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.3099, -0.1811,  0.6901])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Tensors\n",
    "\n",
    "RANDOM_TENSOR1 = torch.randn(3)  # 创建随机张量(3)\n",
    "RANDOM_TENSOR1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2ee861f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_TENSOR1.ndim  # 获取随机张量的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58231363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_TENSOR1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e5c4844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4556, -0.4837,  0.3098,  0.8422],\n",
       "        [-0.5968,  0.2964,  0.6528,  0.9814],\n",
       "        [ 0.4655,  0.6312,  0.2195, -2.0478]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Tensors\n",
    "\n",
    "RANDOM_TENSOR2 = torch.randn(3,4)  # 创建随机张量(3*4),也可以是torch.randn(size=(3,4))\n",
    "RANDOM_TENSOR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4af56d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_TENSOR2.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "027d2e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_TENSOR2.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49a2451d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0378, -0.4633, -1.7917],\n",
       "         [ 0.3666,  1.2363,  0.5686],\n",
       "         [-0.0153,  0.8491,  1.2713]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Tensors\n",
    "\n",
    "RANDOM_TENSOR3 = torch.randn(1,3,3)  # 创建随机张量(1*3*3)\n",
    "RANDOM_TENSOR3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "485ea3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_TENSOR3.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b142374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_TENSOR3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4df0be",
   "metadata": {},
   "source": [
    "### 创建图像大小的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "763d8fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([224, 224, 3]), 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tensor_image_size = torch.randn(224,224,3)  # height, width, color_channels(Red, Green, Blue)\n",
    "random_tensor_image_size.shape, random_tensor_image_size.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd262cbc",
   "metadata": {},
   "source": [
    "### 0-1 张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "846a5805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a tensor of zeros(常用于mask)\n",
    "zeros = torch.zeros(3,4)    # 创建一个2*3的全0张量\n",
    "zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf613f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -0., 0., 0.],\n",
       "        [-0., 0., 0., 0.],\n",
       "        [0., 0., 0., -0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros * RANDOM_TENSOR2 # 全0张量与任意张量相乘，结果仍为全0张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63ef293c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a tensor of ones\n",
    "ones = torch.ones(3,4)\n",
    "ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b9c7cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32, torch.float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones.dtype,zeros.dtype,RANDOM_TENSOR2.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64852722",
   "metadata": {},
   "source": [
    "### 范围张量和相似张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37ff0982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a range of tensors\n",
    "one_to_ten = torch.arange(0,10)  # 创建一个范围张量，从0到9\n",
    "one_to_ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f313d5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a range of tensors\n",
    "Q1 = torch.arange(start =1, end =11, step=1)  # 创建一个范围张量，从1到10\n",
    "Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "269abd55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5, 7, 9])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a range of tensors\n",
    "Q2 = torch.arange(start =1, end =11, step=2)  # 创建一个范围张量，从1到10\n",
    "Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a83b66a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a tensor like\n",
    "Q3 = torch.ones_like(one_to_ten) # 创建一个与one_to_ten形状相同的全1张量，也可以使用torch.ones_like(input=one_to_ten)\n",
    "Q4 = torch.zeros_like(one_to_ten) # 创建一个与one_to_ten形状相同的全0张量，也可以使用torch.zeros_like(input=one_to_ten)\n",
    "Q3, Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a2c2f6",
   "metadata": {},
   "source": [
    "### 张量数据类型（tensor datatypes）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d32be9",
   "metadata": {},
   "source": [
    "**Note**: Tensor datatypes is one of the 3 big errors you'll run into with Pytorch & Deep Learning:\n",
    "\n",
    "1. Tensors not right datatype (e.g. compute float16 with float32)\n",
    "   \n",
    "2. Tensors not right shape\n",
    "   \n",
    "3. Tensors not ont the right (same) device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "24a0f6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3.]), torch.float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Float 32 Tensor\n",
    "float_32_tensor = torch.tensor([1.0 , 2.0 , 3.0], \n",
    "                               dtype=None, # What datatype do you want? float32, float16, int8, etc.\n",
    "                               device=None, # If you want to put tensor on cpu(default), assign device='cpu', if on gpu, device='cuda'\n",
    "                               requires_grad=False # whether or not to track gradients with this tensors operations\n",
    "                               )  # 默认是float32\n",
    "float_32_tensor, float_32_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b43b4975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3.]), torch.float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Float 32 Tensor\n",
    "float_32_tensor = torch.tensor([1.0 , 2.0 , 3.0], dtype=None)  # 默认是float32\n",
    "float_32_tensor, float_32_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4056beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3.]), torch.float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Float 32 Tensor\n",
    "float_32_tensor = torch.tensor([1.0 , 2.0 , 3.0], dtype=torch.float32) \n",
    "float_32_tensor, float_32_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd94c1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3.], dtype=torch.float16), torch.float16)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform float32 to float16\n",
    "float_16_tensor=float_32_tensor.type(torch.float16)\n",
    "float_16_tensor, float_16_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4dbdece2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3.], dtype=torch.float16), torch.float16)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Float 16 Tensor\n",
    "float_16_tensor = torch.tensor([1.0 , 2.0 , 3.0], dtype=torch.float16) \n",
    "float_16_tensor, float_16_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0aa6c3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 4., 9.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute float16 with float32 error\n",
    "float_16_tensor * float_32_tensor # 目前Pytorch的功能强大，可以自动转换dtype，不会报错"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b877ee2e",
   "metadata": {},
   "source": [
    "### Getting information from tensors (tensor attributes)\n",
    "\n",
    "1. Tensors not right datatype (e.g. compute float16 with float32) - to get datatype from tensor , can use `tensor.dtype`\n",
    "   \n",
    "2. Tensors not right shape - to get shape from tensor , can use `tensor.shape`\n",
    "   \n",
    "3. Tensors not ont the right (same) device - to get device from tensor , can use `tensor.device`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af6e9b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datatype of tensor_example: torch.float32\n",
      "Shape of tensor_example: torch.Size([3, 4])\n",
      "Device of tensor_example: cuda:0\n",
      "Size of tensor_example: torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "# create a tensor \n",
    "tensor_example = torch.randn(3,4,device='cuda')\n",
    "print(f\"Datatype of tensor_example: {tensor_example.dtype}\")\n",
    "print(f\"Shape of tensor_example: {tensor_example.shape}\")\n",
    "print(f\"Device of tensor_example: {tensor_example.device}\")\n",
    "print(f\"Size of tensor_example: {tensor_example.size()}\") #tensor.size() 和 tensor.shape 功能相同"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefb959f",
   "metadata": {},
   "source": [
    "### Manipulating Tensors (tensor operations)\n",
    "\n",
    "Tensor operations include:\n",
    "* Addition - 加法\n",
    "  \n",
    "* Subtraction - 减法\n",
    "  \n",
    "* Multiplication (element-wise) - 元素乘法\n",
    "  \n",
    "* Division - 除法\n",
    "  \n",
    "* Martix multiplication - 矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "97de10a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 12, 13])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Addition\n",
    "# create a tensor and add 10 to it\n",
    "tensor_A = torch.tensor([1,2,3])\n",
    "tensor_A + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a2a079ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-9, -8, -7])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subtraction\n",
    "# create a tensor and subtract 10 from it\n",
    "tensor_A - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0428cc52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiplication\n",
    "# create a tensor and multiply by 10\n",
    "tensor_A * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9234c5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.2000, 0.3000])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Division\n",
    "# create a tensor and divide by 10\n",
    "tensor_A / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0d02760b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiplication result: tensor([10, 20, 30])\n",
      "Addition result: tensor([11, 12, 13])\n",
      "Subtraction result: tensor([-9, -8, -7])\n",
      "Division result: tensor([0.1000, 0.2000, 0.3000])\n"
     ]
    }
   ],
   "source": [
    "# Try out Pytorch in-built functions\n",
    "torch.mul(tensor_A,10) # as same as tensor_A * 10\n",
    "print(f\"Multiplication result: {torch.mul(tensor_A,10)}\")\n",
    "torch.add(tensor_A,10) # as same as tensor_A + 10\n",
    "print(f\"Addition result: {torch.add(tensor_A,10)}\")\n",
    "torch.sub(tensor_A,10) # as same as tensor_A - 10\n",
    "print(f\"Subtraction result: {torch.sub(tensor_A,10)}\")\n",
    "torch.div(tensor_A,10) # as same as tensor_A / 10\n",
    "print(f\"Division result: {torch.div(tensor_A,10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49d1f7f",
   "metadata": {},
   "source": [
    "### MARTIX Multiplication (矩阵乘法)\n",
    "\n",
    "Two main ways of performing multiplacation in neural networks and deep learning:\n",
    "\n",
    "1. Element-wise multiplication\n",
    "   \n",
    "2. Martix multiplication（dot product————点积）\n",
    "\n",
    "There are two main rules that performing matrix mutliplication needs to satisfy:\n",
    "1. The **inner dimensions** must match:\n",
    "* `(3, 2) @ (3, 2)` won't work\n",
    "* `(2, 3) @ (3, 2)` will work\n",
    "* `(3, 2) @ (2, 3)` will work\n",
    "2. The resulting matrix has the shape of the **outer dimensions**:\n",
    "* `(2, 3) @ (3, 2)` -> `(2, 2)`\n",
    "* `(3, 2) @ (2, 3)` -> `(3, 3)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "49eb3baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) * tensor([1, 2, 3])\n",
      "Equals: tensor([1, 4, 9])\n",
      "Equals: tensor([1, 4, 9])\n"
     ]
    }
   ],
   "source": [
    "# Element wise Multiplication (矩阵乘法————逐元素相乘)\n",
    "print(tensor_A, \"*\" , tensor_A)\n",
    "print(f\"Equals: {tensor_A * tensor_A}\")\n",
    "print(f\"Equals: {torch.mul(tensor_A, tensor_A)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8812cf53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) · tensor([1, 2, 3])\n",
      "Equals: 14\n",
      "Equals: 14\n"
     ]
    }
   ],
   "source": [
    "# Matrix Multiplication (矩阵乘法———点积)\n",
    "print(tensor_A, \"·\", tensor_A)\n",
    "print(f\"Equals: {torch.matmul(tensor_A, tensor_A)}\")\n",
    "print(f\"Equals: {tensor_A @ tensor_A}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "434e71c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 μs, sys: 1e+03 ns, total: 15 μs\n",
      "Wall time: 28.8 μs\n",
      "Equals: 14\n"
     ]
    }
   ],
   "source": [
    "# Martix Multiplication by loop\n",
    "%time\n",
    "value = 0\n",
    "for i in range(len(tensor_A)):\n",
    "    value += tensor_A[i] * tensor_A[i]\n",
    "print(f\"Equals: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f74ab974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19 μs, sys: 0 ns, total: 19 μs\n",
      "Wall time: 35.3 μs\n",
      "Equals: 14\n"
     ]
    }
   ],
   "source": [
    "# Martix Multiplication by torch in-built function\n",
    "%time\n",
    "value = torch.matmul(tensor_A, tensor_A)\n",
    "print(f\"Equals: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a26acd4",
   "metadata": {},
   "source": [
    "**Note**: 很明显内置函数的运算更快\n",
    "\n",
    "```python\n",
    "value = 0\n",
    "for i in range(1, 11):\n",
    "    value += i\n",
    "print(value)\n",
    "```\n",
    "\n",
    "循环基本写法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4daa0b6",
   "metadata": {},
   "source": [
    "**Note**：\n",
    "\n",
    "逐元素相乘：对应位置一一相乘，记作 A ⊙ B。\n",
    "\n",
    "矩阵乘法：按行×列做点积，记作 A @ B 或 AB。\n",
    "\n",
    "PyTorch 运算符\n",
    "\n",
    "逐元素：* 或 torch.mul\n",
    "\n",
    "矩阵乘法：@、torch.matmul、二维专用 torch.mm、批量 torch.bmm\n",
    "\n",
    "形状规则\n",
    "\n",
    "逐元素：对应维度要相等，或能按广播规则扩展（某维为 1 或缺失可广播）。\n",
    "\n",
    "矩阵乘法：内维必须匹配，即 (m, n) @ (n, p) -> (m, p)；批量维可按规则广播，但参与乘法的最后两维需满足上述条件。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05b8079",
   "metadata": {},
   "source": [
    "### One of the most common errors in deep learning : shape errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4535c75e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      2\u001b[39m tensor_1 = torch.tensor([[\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m],\n\u001b[32m      3\u001b[39m                          [\u001b[32m3\u001b[39m,\u001b[32m4\u001b[39m],\n\u001b[32m      4\u001b[39m                          [\u001b[32m5\u001b[39m,\u001b[32m6\u001b[39m]])\n\u001b[32m      6\u001b[39m tensor_2 = torch.tensor([[\u001b[32m7\u001b[39m,\u001b[32m10\u001b[39m],\n\u001b[32m      7\u001b[39m                          [\u001b[32m8\u001b[39m,\u001b[32m11\u001b[39m],\n\u001b[32m      8\u001b[39m                          [\u001b[32m9\u001b[39m,\u001b[32m12\u001b[39m]])\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_2\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# tensor_2.T表示对tensor_2进行转置\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)"
     ]
    }
   ],
   "source": [
    "# Shape for martix multiplication\n",
    "tensor_1 = torch.tensor([[1,2],\n",
    "                         [3,4],\n",
    "                         [5,6]])\n",
    "\n",
    "tensor_2 = torch.tensor([[7,10],\n",
    "                         [8,11],\n",
    "                         [9,12]])\n",
    "torch.mm(tensor_1, tensor_2)  # tensor_2.T表示对tensor_2进行转置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb57afb9",
   "metadata": {},
   "source": [
    "To fix our tensor shape issues, we can manipulate the shape of one of our tensors using a **transpose**.\n",
    "\n",
    "A **transpose** switches the axes or dimensions of a given tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432b506c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor_2 before transpose: torch.Size([3, 2])\n",
      "Shape of tensor_2 after transpose: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of tensor_2 before transpose: {tensor_2.shape}\")\n",
    "print(f\"Shape of tensor_2 after transpose: {tensor_2.T.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacb07ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shapes: tensor_1 = torch.Size([3, 2]), tensor_2 = torch.Size([3, 2])\n",
      "New shapes: tensor_1 = torch.Size([3, 2]) (same shape as above), tensor_2.T = torch.Size([2, 3])\n",
      "Multiplying: torch.Size([3, 2]) @ torch.Size([2, 3]) <- inner dimensions must match\n",
      "Output:\n",
      "\n",
      "tensor([[ 27,  30,  33],\n",
      "        [ 61,  68,  75],\n",
      "        [ 95, 106, 117]])\n",
      "\n",
      "Output shape: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# The matrix multiplication operation works when tensor_2 is transposed\n",
    "print(f\"Original shapes: tensor_1 = {tensor_1.shape}, tensor_2 = {tensor_2.shape}\")\n",
    "print(f\"New shapes: tensor_1 = {tensor_1.shape} (same shape as above), tensor_2.T = {tensor_2.T.shape}\")\n",
    "print(f\"Multiplying: {tensor_1.shape} @ {tensor_2.T.shape} <- inner dimensions must match\")\n",
    "print(\"Output:\\n\")\n",
    "output = torch.matmul(tensor_1, tensor_2.T)\n",
    "print(output) \n",
    "print(f\"\\nOutput shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc1161f",
   "metadata": {},
   "source": [
    "### Element Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc962f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim of x: 2\n",
      "\n",
      "Shape of x: torch.Size([2, 2])\n",
      "\n",
      "dim of y: 2\n",
      "\n",
      "Shape of y: torch.Size([2, 2])\n",
      "\n",
      "Shape of out: torch.Size([2, 2])\n",
      "\n",
      "Contents of out: tensor([[ 5, 12],\n",
      "        [21, 32]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2],\n",
    "                 [3,4]])\n",
    "\n",
    "y= torch.tensor([[5,6],\n",
    "               [7,8]])\n",
    "\n",
    "out = x*y\n",
    "print(f\"dim of x: {x.ndim}\\n\")\n",
    "print(f\"Shape of x: {x.shape}\\n\")\n",
    "print(f\"dim of y: {y.ndim}\\n\")\n",
    "print(f\"Shape of y: {y.shape}\\n\")\n",
    "print(f\"Shape of out: {out.shape}\\n\")\n",
    "print(f\"Contents of out: {out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d80769",
   "metadata": {},
   "source": [
    "**Note**: 形状完全一样，可以逐元素相乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a6e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim of x: 2\n",
      "\n",
      "Shape of x: torch.Size([2, 2])\n",
      "\n",
      "dim of y: 1\n",
      "\n",
      "Shape of y: torch.Size([2])\n",
      "\n",
      "Shape of out: torch.Size([2, 2])\n",
      "\n",
      "Contents of out: tensor([[ 5, 12],\n",
      "        [15, 24]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2],\n",
    "                 [3,4]])\n",
    "\n",
    "y= torch.tensor([5,6])\n",
    "\n",
    "out = x*y\n",
    "print(f\"dim of x: {x.ndim}\\n\")\n",
    "print(f\"Shape of x: {x.shape}\\n\")\n",
    "print(f\"dim of y: {y.ndim}\\n\")\n",
    "print(f\"Shape of y: {y.shape}\\n\")\n",
    "print(f\"Shape of out: {out.shape}\\n\")\n",
    "print(f\"Contents of out: {out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7e1852",
   "metadata": {},
   "source": [
    "**Note**: 行对应，可以逐元素相乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54213483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim of x: 2\n",
      "\n",
      "Shape of x: torch.Size([2, 2])\n",
      "\n",
      "dim of y: 2\n",
      "\n",
      "Shape of y: torch.Size([2, 1])\n",
      "\n",
      "Shape of out: torch.Size([2, 2])\n",
      "\n",
      "Contents of out: tensor([[ 5, 10],\n",
      "        [18, 24]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2],\n",
    "                 [3,4]])\n",
    "\n",
    "y= torch.tensor([[5],\n",
    "                 [6]])\n",
    "\n",
    "out = x*y\n",
    "print(f\"dim of x: {x.ndim}\\n\")\n",
    "print(f\"Shape of x: {x.shape}\\n\")\n",
    "print(f\"dim of y: {y.ndim}\\n\")\n",
    "print(f\"Shape of y: {y.shape}\\n\")\n",
    "print(f\"Shape of out: {out.shape}\\n\")\n",
    "print(f\"Contents of out: {out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5740d9",
   "metadata": {},
   "source": [
    "**Note**: 列对应，可以逐元素相乘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2237c6d1",
   "metadata": {},
   "source": [
    "### Finding the min,max,mean,sum,etc (tensor aggregation(张量聚合))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fac552a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor\n",
    "x = torch.arange(0,100,10)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "76a7b26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min value in tensor x is : 0\n"
     ]
    }
   ],
   "source": [
    "# Find the min value in a tensor\n",
    "min_value = torch.min(x) # also x.min()\n",
    "print(f\"Min value in tensor x is : {min_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ca2ee931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value in tensor x is : 90\n"
     ]
    }
   ],
   "source": [
    "# Find the max value in a tensor\n",
    "max_value = torch.max(x) # also x.max()\n",
    "print(f\"Max value in tensor x is : {max_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e700c8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean value in tensor x is : 45.0\n"
     ]
    }
   ],
   "source": [
    "# Find the mean value in a tensor\n",
    "mean_value = torch.mean(x.type(torch.float32))  # 注意：要先将整数张量转换为浮点型张量才能计算均值,also x.type(torch.float32).mean()\n",
    "print(f\"Mean value in tensor x is : {mean_value}\")  # dtype张量转换为浮点型张量才能计算均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "21bc4769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of all values in tensor x is : 450\n"
     ]
    }
   ],
   "source": [
    "# compute the sum of all values in a tensor\n",
    "sum_value = torch.sum(x) # also x.sum()\n",
    "print(f\"Sum of all values in tensor x is : {sum_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffe4939",
   "metadata": {},
   "source": [
    "### Finding the positional min and max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2066c695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position of the max value in tensor x is : 9\n"
     ]
    }
   ],
   "source": [
    "# Find the position of the max element in a tensor\n",
    "max_position = torch.argmax(x) # also x.argmax()\n",
    "print(f\"Position of the max value in tensor x is : {max_position}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "64dc9b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position of the min value in tensor x is : 0\n"
     ]
    }
   ],
   "source": [
    "# Find the position of the min element in a tensor \n",
    "min_position = torch.argmin(x) # also x.argmin()\n",
    "print(f\"Position of the min value in tensor x is : {min_position}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3535ed2d",
   "metadata": {},
   "source": [
    "### Reshaping,stacking,squeezing and unsqueezing tensors (重塑，堆叠，压缩和解压缩)\n",
    "\n",
    "* Reshaping - reshapes an input tensor to a defined shape（把输入张量重塑成特定形状）\n",
    "  \n",
    "* View - Return a view of an input tensor of certain shape but keep the same memory as the original tensor（返回张量特定形状的视图，但不改变张量本身）\n",
    "  \n",
    "* Stacking - combine multiple tensors on top of each other (vstack) or side by side (hstack)（将多个张量堆叠）\n",
    "  \n",
    "* Squeeze - removes all `single` dimensions from a tensor\n",
    "  \n",
    "* Unsqueeze - add a `single` dimension to a target tensor\n",
    "  \n",
    "* Permute - Return a view of the input with dimensions permuted (swapped) in a certain way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2b43cb4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.]), torch.Size([9]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor\n",
    "x = torch.arange(1.,10.)\n",
    "x,x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "baa165a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_shaped1: tensor([[1., 2., 3., 4., 5., 6., 7., 8., 9.]]), torch.Size([1, 9])\n",
      "x_shaped2: tensor([[1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.],\n",
      "        [6.],\n",
      "        [7.],\n",
      "        [8.],\n",
      "        [9.]]), torch.Size([9, 1])\n",
      "x_shaped3: tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]]), torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Reshape your tensor\n",
    "x_shaped1 = x.reshape(1,9) # 元素数量要匹配\n",
    "x_shaped2 = x.reshape(9,1) # 元素数量要匹配\n",
    "x_shaped3 = x.reshape(3,3) # 元素数量要匹配\n",
    "print(f\"x_shaped1: {x_shaped1}, {x_shaped1.shape}\")\n",
    "print(f\"x_shaped2: {x_shaped2}, {x_shaped2.shape}\")\n",
    "print(f\"x_shaped3: {x_shaped3}, {x_shaped3.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cc51a813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3., 4., 5., 6., 7., 8., 9.]]), torch.Size([1, 9]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the view of a tensor\n",
    "z = x.view(1,9)\n",
    "z, z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7f4681e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified z: tensor([[5., 2., 3., 4., 5., 6., 7., 8., 9.]])\n",
      "Original x after modifying z: tensor([5., 2., 3., 4., 5., 6., 7., 8., 9.])\n"
     ]
    }
   ],
   "source": [
    "# Changing z changes x (because a view of a tensor shares the same memory as the original input tensor)(z 和 x 共享内存，改变z会影响x)\n",
    "z[:,0] = 5\n",
    "print(f\"Modified z: {z}\")\n",
    "print(f\"Original x after modifying z: {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7ea0aa34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的第三列: tensor([3, 6])\n",
      "w的第二行: tensor([4, 5, 6])\n",
      "w的第二行第三列: 6\n"
     ]
    }
   ],
   "source": [
    "# Indexing (索引)\n",
    "w = torch.tensor([[1,2,3],\n",
    "                  [4,5,6]])\n",
    "print(f\"w的第三列: {w[:,2]}\")\n",
    "print(f\"w的第二行: {w[1,:]}\")\n",
    "print(f\"w的第二行第三列: {w[1,2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "24e7b3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
       "         [5., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
       "         [5., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
       "         [5., 2., 3., 4., 5., 6., 7., 8., 9.]]),\n",
       " torch.Size([4, 9]),\n",
       " tensor([[5., 5., 5., 5.],\n",
       "         [2., 2., 2., 2.],\n",
       "         [3., 3., 3., 3.],\n",
       "         [4., 4., 4., 4.],\n",
       "         [5., 5., 5., 5.],\n",
       "         [6., 6., 6., 6.],\n",
       "         [7., 7., 7., 7.],\n",
       "         [8., 8., 8., 8.],\n",
       "         [9., 9., 9., 9.]]),\n",
       " torch.Size([9, 4]),\n",
       " tensor([[5., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
       "         [5., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
       "         [5., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
       "         [5., 2., 3., 4., 5., 6., 7., 8., 9.]]),\n",
       " torch.Size([4, 9]),\n",
       " tensor([5., 2., 3., 4., 5., 6., 7., 8., 9., 5., 2., 3., 4., 5., 6., 7., 8., 9.,\n",
       "         5., 2., 3., 4., 5., 6., 7., 8., 9., 5., 2., 3., 4., 5., 6., 7., 8., 9.]),\n",
       " torch.Size([36]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stack tensors on top \n",
    "x_stacked_0 = torch.stack([x,x,x,x])  # 默认在第0维度进行堆叠,即在最外层增加一个维度，torch.stack([<tensors>], dim=0)\n",
    "x_stacked_1 = torch.stack([x,x,x,x], dim=1)  # 在第1维度进行堆叠\n",
    "x_stacked_v = torch.vstack([x,x,x,x])  # 在第0维度进行堆叠\n",
    "x_stacked_h = torch.hstack([x,x,x,x])  # 横向堆叠\n",
    "x_stacked_0, x_stacked_0.shape,x_stacked_1, x_stacked_1.shape,x_stacked_v,x_stacked_v.shape,x_stacked_h, x_stacked_h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e363aa",
   "metadata": {},
   "source": [
    "**Note**: \n",
    "\n",
    "当输入是 1D 张量时：\n",
    "\n",
    "vstack([x, x]) 与 stack([x, x], dim=0) 结果形状一致，常可视为等价。\n",
    "\n",
    "例如 x: (4,) → vstack → (2, 4)，stack(dim=0) → (2, 4)。\n",
    "\n",
    "当输入是 2D 或更高维张量时：\n",
    "\n",
    "不等价。vstack 相当于按行拼接（cat(dim=0)），不会新增维度；\n",
    "\n",
    "而 stack 会在指定位置新增一个新维度，形状会不同。\n",
    "\n",
    "用形状直观对比\n",
    "\n",
    "1D 情况（与你笔记本里的 x 一样是 1D）：\n",
    "\n",
    "x: (9,)；vstack([x,x,x,x]) → (4, 9)\n",
    "\n",
    "stack([x,x,x,x], dim=0) → (4, 9) ← 与 vstack 相同\n",
    "\n",
    "stack([x,x,x,x], dim=1) → (9, 4) ← 新增列维\n",
    "\n",
    "2D 情况（A、B: (3, 4)）：\n",
    "\n",
    "vstack([A, B]) → (6, 4) ← 沿第 0 维拼接\n",
    "\n",
    "stack([A, B], dim=0) → (2, 3, 4) ← 新增一个维度\n",
    "\n",
    "该用哪个:\n",
    "\n",
    "想“把行接在一起”（样本拼接/增大批量）：用 vstack 或 cat(dim=0)。\n",
    "\n",
    "想“打包成一个新维度”（增加批次维、头数维等）：用 stack(dim=…)，它会新增一个维度。\n",
    "\n",
    "小结:\n",
    "\n",
    "vstack ≈ cat(dim=0)，但会把 1D 张量先提升为 2D 的行向量后再拼接；\n",
    "\n",
    "hstack ≈ cat(dim=1)，对 1D 会当作列方向拼接；\n",
    "\n",
    "stack(dim=k) 会“新增”一个维度，语义和结果与 vstack/hstack 不同（除 1D 特例外观一致）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f66313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1, 2, 3],\n",
       "          [4, 5, 6]],\n",
       " \n",
       "         [[1, 2, 3],\n",
       "          [4, 5, 6]]]),\n",
       " tensor([[1, 2, 3],\n",
       "         [4, 5, 6],\n",
       "         [1, 2, 3],\n",
       "         [4, 5, 6]]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 例如\n",
    "w_1 = torch.stack([w,w])\n",
    "w_2 = torch.vstack([w,w])\n",
    "w_1,w_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeb4278",
   "metadata": {},
   "source": [
    "### 用最通俗的话理解“维度”（dimension）和 shape\n",
    "\n",
    "- 维度是什么？\n",
    "  - 可以理解成“有几条互相独立的方向/轴”。\n",
    "  - 标量=0维（只有一个数）；向量=1维（像一条数轴上的一排数）；矩阵=2维（有行和列的表格）；更高维=在此基础上再套一层层“外壳”。\n",
    "\n",
    "- shape 是什么？\n",
    "  - shape 就是“每个维度上的长度”。\n",
    "  - 例如 (3, 4) 表示 2D：3 行、4 列；(2, 3, 4) 表示 3D：最外层有 2 个“页”，每页是 3×4 的矩阵。\n",
    "  - 在深度学习里常见的形状如 (N, C, H, W)：批次 N、通道 C、高 H、宽 W（PyTorch 常用 channels-first）。\n",
    "\n",
    "- 常见操作如何改变维度/shape\n",
    "  - stack：在指定位置“新增一个维度”。比如把多个同形状张量打包成一叠。\n",
    "  - vstack/hstack：不新增维度，只是在现有维度上“接在一起”（v=竖直按行拼，h=水平按列拼）。\n",
    "  - unsqueeze/squeeze：显式地“加/去”长度为 1 的维度。\n",
    "  - reshape：改变形状但元素总数不变（像把乐高换个拼法）。\n",
    "  - transpose/permute：交换维度顺序（把行列或更高维的轴对调）。\n",
    "\n",
    "- 一句话技巧\n",
    "  - 看 shape 从左到右就像坐标：(外层 … -> 内层)。矩阵乘法要“内维对得上（m,n)@(n,p)→(m,p)”，堆叠/拼接要“外观一致或能广播”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4624f010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1D 示例 ---\n",
      "x: torch.Size([5])\n",
      "stack(dim=0): torch.Size([2, 5])\n",
      "vstack:       torch.Size([2, 5])\n",
      "hstack:       torch.Size([10])\n",
      "unsqueeze(0): torch.Size([1, 5])\n",
      "unsqueeze(1): torch.Size([5, 1])\n",
      "\n",
      "--- 2D 示例 ---\n",
      "A: torch.Size([2, 3]) B: torch.Size([2, 3])\n",
      "vstack:       torch.Size([4, 3])\n",
      "cat(dim=0):  torch.Size([4, 3])\n",
      "stack(dim=0): torch.Size([2, 2, 3])\n",
      "hstack:       torch.Size([2, 6])\n",
      "cat(dim=1):  torch.Size([2, 6])\n",
      "stack(dim=1): torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# 维度/shape 快速可视化演示\n",
    "import torch\n",
    "\n",
    "print(\"--- 1D 示例 ---\")\n",
    "x = torch.arange(1., 6.)          # shape: (5,)\n",
    "print(\"x:\", x.shape)\n",
    "x_stack0 = torch.stack([x, x], dim=0)   # 新增一个维度在最外层 -> (2, 5)\n",
    "x_v = torch.vstack([x, x])              # 按行堆叠 -> (2, 5)（1D 特例与 stack(dim=0) 形状一致）\n",
    "x_h = torch.hstack([x, x])              # 按列拼接 -> (10,)\n",
    "print(\"stack(dim=0):\", x_stack0.shape)\n",
    "print(\"vstack:      \", x_v.shape)\n",
    "print(\"hstack:      \", x_h.shape)\n",
    "\n",
    "# unsqueeze 显式增加长度为1的维度\n",
    "x_u0 = x.unsqueeze(0)   # (1, 5)\n",
    "x_u1 = x.unsqueeze(1)   # (5, 1)\n",
    "print(\"unsqueeze(0):\", x_u0.shape)\n",
    "print(\"unsqueeze(1):\", x_u1.shape)\n",
    "\n",
    "print(\"\\n--- 2D 示例 ---\")\n",
    "A = torch.arange(1., 7.).reshape(2, 3)   # (2, 3)\n",
    "B = torch.arange(10., 16.).reshape(2, 3) # (2, 3)\n",
    "print(\"A:\", A.shape, \"B:\", B.shape)\n",
    "A_v = torch.vstack([A, B])               # (4, 3) ← 沿第0维拼接\n",
    "A_cat0 = torch.cat([A, B], dim=0)        # (4, 3) ← 与 vstack 等价\n",
    "A_s0 = torch.stack([A, B], dim=0)        # (2, 2, 3) ← 新增维度\n",
    "A_h = torch.hstack([A, B])               # (2, 6) ← 沿第1维拼接\n",
    "A_cat1 = torch.cat([A, B], dim=1)        # (2, 6) ← 与 hstack 等价\n",
    "A_s1 = torch.stack([A, B], dim=1)        # (2, 2, 3) ← 在第1维新增维度\n",
    "print(\"vstack:      \", A_v.shape)\n",
    "print(\"cat(dim=0): \", A_cat0.shape)\n",
    "print(\"stack(dim=0):\", A_s0.shape)\n",
    "print(\"hstack:      \", A_h.shape)\n",
    "print(\"cat(dim=1): \", A_cat1.shape)\n",
    "print(\"stack(dim=1):\", A_s1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "438bfcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous tensor: tensor([[5., 2., 3., 4., 5., 6., 7., 8., 9.]])\n",
      "Previous shape: torch.Size([1, 9])\n",
      "\n",
      "New tensor: tensor([5., 2., 3., 4., 5., 6., 7., 8., 9.])\n",
      "New shape: torch.Size([9])\n"
     ]
    }
   ],
   "source": [
    "# torch.squeeze() - removes all single dimensions from a target tensor\n",
    "print(f\"Previous tensor: {x_shaped1}\")\n",
    "print(f\"Previous shape: {x_shaped1.shape}\")\n",
    "\n",
    "# Remove extra dimensions from x_shaped1\n",
    "x_squeezed = x_shaped1.squeeze()\n",
    "print(f\"\\nNew tensor: {x_squeezed}\")\n",
    "print(f\"New shape: {x_squeezed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d1cbb5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous target: tensor([5., 2., 3., 4., 5., 6., 7., 8., 9.])\n",
      "Previous shape: torch.Size([9])\n",
      "\n",
      "New tensor (dim=0): tensor([[5., 2., 3., 4., 5., 6., 7., 8., 9.]])\n",
      "New shape (dim=0): torch.Size([1, 9])\n",
      "\n",
      "New tensor (dim=1): tensor([[5.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.],\n",
      "        [6.],\n",
      "        [7.],\n",
      "        [8.],\n",
      "        [9.]])\n",
      "New shape (dim=1): torch.Size([9, 1])\n"
     ]
    }
   ],
   "source": [
    "# torch.unsqueeze() - adds a single dimension to a target tensor at a specific dim (dimension)\n",
    "print(f\"Previous target: {x_squeezed}\")\n",
    "print(f\"Previous shape: {x_squeezed.shape}\")\n",
    "\n",
    "# Add an extra dimension with unsqueeze\n",
    "x_unsqueezed0 = x_squeezed.unsqueeze(dim=0)\n",
    "x_unsqueezed1 = x_squeezed.unsqueeze(dim=1)\n",
    "print(f\"\\nNew tensor (dim=0): {x_unsqueezed0}\")\n",
    "print(f\"New shape (dim=0): {x_unsqueezed0.shape}\")\n",
    "print(f\"\\nNew tensor (dim=1): {x_unsqueezed1}\")\n",
    "print(f\"New shape (dim=1): {x_unsqueezed1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5690f294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous shape: torch.Size([224, 224, 3])\n",
      "New shape: torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# torch.permute - rearranges the dimensions of a target tensor in a specified order\n",
    "x_original = torch.rand(size=(224, 224, 3)) # [height, width, colour_channels]\n",
    "\n",
    "# Permute the original tensor to rearrange the axis (or dim) order\n",
    "x_permuted = x_original.permute(2, 0, 1) # shifts axis 0->1, 1->2, 2->0\n",
    "\n",
    "print(f\"Previous shape: {x_original.shape}\") \n",
    "print(f\"New shape: {x_permuted.shape}\") # [colour_channels, height, width]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37e290c",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "\n",
    "rand:返回一个张量，包含了从区间[0, 1)的均匀分布中抽取的一组随机数。\n",
    "\n",
    "randn:返回一个张量，包含了从标准正态分布（均值为0，方差为1，即高斯白噪声）中抽取的一组随机数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "67a9e5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[[ 0.4808, -0.2926,  0.9423],\n",
      "         [ 0.9485,  0.9199,  1.0902]]])\n",
      "x_view: tensor([[ 0.4808, -0.2926,  0.9423],\n",
      "        [ 0.9485,  0.9199,  1.0902]])\n",
      "x_permute: tensor([[[ 0.4808,  0.9485]],\n",
      "\n",
      "        [[-0.2926,  0.9199]],\n",
      "\n",
      "        [[ 0.9423,  1.0902]]])\n",
      "x: tensor([[[ 2.0000, -0.2926,  0.9423],\n",
      "         [ 2.0000,  0.9199,  1.0902]]])\n",
      "x_view: tensor([[ 2.0000, -0.2926,  0.9423],\n",
      "        [ 2.0000,  0.9199,  1.0902]])\n",
      "x_permute: tensor([[[ 2.0000,  2.0000]],\n",
      "\n",
      "        [[-0.2926,  0.9199]],\n",
      "\n",
      "        [[ 0.9423,  1.0902]]])\n"
     ]
    }
   ],
   "source": [
    "# torch.permute 类似于torch.view，都与原tensor共享内存，不同之处在于permute可以改变维度的顺序，而view只能改变形状但不能改变维度的顺序。\n",
    "x = torch.randn(1,2,3)\n",
    "x_view = x.view(2,3)\n",
    "x_permute = x.permute(2,0,1)  # 交换维度\n",
    "print(f\"x: {x}\")\n",
    "print(f\"x_view: {x_view}\")\n",
    "print(f\"x_permute: {x_permute}\")\n",
    "x_permute[0,:,:] = 2 # 可以看到，修改x_permute会影响x，也不会影响x_view\n",
    "print(f\"x: {x}\")\n",
    "print(f\"x_view: {x_view}\")\n",
    "print(f\"x_permute: {x_permute}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297a978e",
   "metadata": {},
   "source": [
    "## Indexing (selecting data from tensors)\n",
    "\n",
    "Indexing with PyTorch is similar to indexing with NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "42f4a1bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1,  2,  3],\n",
       "          [ 4,  5,  6],\n",
       "          [ 7,  8,  9]],\n",
       " \n",
       "         [[10, 11, 12],\n",
       "          [13, 14, 15],\n",
       "          [16, 17, 18]]]),\n",
       " torch.Size([2, 3, 3]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor\n",
    "import torch\n",
    "x = torch.arange(1, 19).reshape(2, 3, 3)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c5454887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's index on our new tensor\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "db65aaa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's index on the middle bracket (dim=1)\n",
    "x[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8e9e1ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's index on the most inner bracket (last dimension)\n",
    "x[0][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "750119da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3],\n",
       "        [10, 11, 12]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5173c256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  5,  8],\n",
       "        [11, 14, 17]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all values of 0th and 1st dimensions but only index 1 of 2nd dimension\n",
    "x[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4e27ccda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 14])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all values of the 0 dimension but only the 1 index value of 1st and 2nd dimension\n",
    "x[:, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "51bc4e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get index 0 of 0th and 1st dimension and all values of 2nd dimension\n",
    "x[0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1b452224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9)\n",
      "tensor([3, 6, 9])\n"
     ]
    }
   ],
   "source": [
    "# Index on x to return 9\n",
    "print(x[0][2][2])\n",
    "\n",
    "# Index on x to return 3, 6, 9\n",
    "print(x[0, :, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55a7c63",
   "metadata": {},
   "source": [
    "## PyTorch tensors & NumPy\n",
    "\n",
    "NumPy is a popular scientific Python numerical computing library. \n",
    "\n",
    "And because of this, PyTorch has functionality to interact with it.\n",
    "\n",
    "* Data in NumPy, want in PyTorch tensor -> `torch.from_numpy(ndarray)`\n",
    "  \n",
    "* PyTorch tensor -> NumPy -> `torch.Tensor.numpy()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "4a7f2b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array: [1. 2. 3. 4. 5. 6. 7. 8. 9.], dtype: float64\n",
      "tensor: tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=torch.float64), dtype: torch.float64\n",
      "Converted tensor: tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.]), dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# NumPy array to tensor\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "array = np.arange(1.,10.)\n",
    "tensor = torch.from_numpy(array) # warning: when converting from numpy -> pytorch, pytorch reflects numpy's default datatype of float64 unless specified otherwise\n",
    "print(f\"array: {array}, dtype: {array.dtype}\")\n",
    "print(f\"tensor: {tensor}, dtype: {tensor.dtype}\")\n",
    "tensor = tensor.type(torch.float32)  # convert to float32\n",
    "print(f\"Converted tensor: {tensor}, dtype: {tensor.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3881ab",
   "metadata": {},
   "source": [
    "**Note**: \n",
    "\n",
    "Numpy的数组默认是float64格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b5fb8208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]),\n",
       " tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.]))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the value of array, what will this do to `tensor`?\n",
    "array = array + 1\n",
    "array, tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6f0374a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor: tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.]), dtype: torch.float32\n",
      "array: [1. 2. 3. 4. 5. 6. 7. 8. 9.], dtype: float32\n",
      "Converted array: [1. 2. 3. 4. 5. 6. 7. 8. 9.], dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# tensor to NumPy array\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "tensor = torch.arange(1.,10.)\n",
    "array = tensor.numpy() # warning: when converting from pytorch -> numpy, numpy reflects pytorch's default datatype unless specified otherwise\n",
    "print(f\"tensor: {tensor}, dtype: {tensor.dtype}\")\n",
    "print(f\"array: {array}, dtype: {array.dtype}\")\n",
    "array = array.astype(np.float64) # numpy转换格式用astype\n",
    "print(f\"Converted array: {array}, dtype: {array.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3259f8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]),\n",
       " array([1., 2., 3., 4., 5., 6., 7., 8., 9.]))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the value of tensor, what will this do to `array`?\n",
    "tensor = tensor + 1\n",
    "tensor, array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1fba40",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "\n",
    "array 和 tensor 不共享内存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baecdab8",
   "metadata": {},
   "source": [
    "## Reproducbility (trying to take random out of random)\n",
    "\n",
    "In short how a neural network learns:\n",
    "\n",
    "`start with random numbers -> tensor operations -> update random numbers to try and make them better representations of the data -> again -> again -> again...`\n",
    "\n",
    "To reduce the randomness in neural networks and PyTorch comes the concept of a **random seed**.\n",
    "\n",
    "Essentially what the random seed does is \"flavour\" the randomness. （随机种子的作用是影响随机性）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6f56d9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9409, 0.8103, 0.7426, 0.6611],\n",
      "        [0.7813, 0.2003, 0.7085, 0.2656],\n",
      "        [0.1189, 0.5953, 0.6250, 0.3385]])\n",
      "tensor([[0.9298, 0.8004, 0.4918, 0.8913],\n",
      "        [0.9207, 0.3757, 0.1219, 0.7395],\n",
      "        [0.3519, 0.1420, 0.3500, 0.6632]])\n",
      "tensor([[False, False, False, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create two random tensors\n",
    "random_tensor_A = torch.rand(3, 4)\n",
    "random_tensor_B = torch.rand(3, 4)\n",
    "\n",
    "print(random_tensor_A)\n",
    "print(random_tensor_B)\n",
    "print(random_tensor_A == random_tensor_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e33218ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "tensor([[True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "# Let's make some random but reproducible tensors\n",
    "import torch\n",
    "\n",
    "# Set the random seed\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random_tensor_C = torch.rand(3, 4)\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random_tensor_D = torch.rand(3, 4)\n",
    "\n",
    "print(random_tensor_C)\n",
    "print(random_tensor_D)\n",
    "print(random_tensor_C == random_tensor_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296fad6a",
   "metadata": {},
   "source": [
    " ## Running tensors and PyTorch objects on the GPUs (and making faster computations)\n",
    "\n",
    " GPUs = faster computation on numbers, thanks to CUDA + NVIDIA hardware + PyTorch working behind the scenes to make everything hunky dory (good)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9281a749",
   "metadata": {},
   "source": [
    "### 1. Getting a GPU\n",
    "\n",
    "1. Easiest - Use Google Colab for a free GPU (options to upgrade as well)\n",
    "   \n",
    "3. Use your own GPU - takes a little bit of setup and requires the investment of purchasing a GPU, there's lots of options..., see this post for what option to get: https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/\n",
    "   \n",
    "4. Use cloud computing - GCP, AWS, Azure, these services allow you to rent computers on the cloud and access them\n",
    "\n",
    "For 2, 3 PyTorch + GPU drivers (CUDA) takes a little bit of setting up, to do this, refer to PyTorch setup documentation: https://pytorch.org/get-started/locally/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f437c974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct 26 17:22:53 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4080 ...    Off |   00000000:18:00.0 Off |                  N/A |\n",
      "| 38%   36C    P8             21W /  320W |     266MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4080 ...    Off |   00000000:5E:00.0 Off |                  N/A |\n",
      "| 38%   37C    P8              9W /  320W |      13MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 4080 ...    Off |   00000000:86:00.0 Off |                  N/A |\n",
      "| 38%   39C    P8             20W /  320W |      13MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 4080 ...    Off |   00000000:AF:00.0 Off |                  N/A |\n",
      "| 38%   40C    P8             18W /  320W |      13MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     46105      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    0   N/A  N/A    971831      C   ...q/anaconda3/envs/pytorch/bin/python        250MiB |\n",
      "|    1   N/A  N/A     46105      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    2   N/A  N/A     46105      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    3   N/A  N/A     46105      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18f67d9",
   "metadata": {},
   "source": [
    "### 2. Check for GPU access with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "03cc7aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for GPU access with PyTorch\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c750440b",
   "metadata": {},
   "source": [
    "For PyTorch since it's capable of running compute on the GPU or CPU, it's best practice to setup device agnostic code: https://pytorch.org/docs/stable/notes/cuda.html#best-practices\n",
    "\n",
    "E.g. run on GPU if available, else default to CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "21bfb187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup device agnostic code \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e6e5e66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count number of devices\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d429bb9b",
   "metadata": {},
   "source": [
    "## 3. Putting tensors (and models) on the GPU\n",
    "\n",
    "The reason we want our tensors/models on the GPU is because using a GPU results in faster computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "eda5ce7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) cpu\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor (default on the CPU)\n",
    "tensor = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Tensor not on GPU\n",
    "print(tensor, tensor.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "671b6d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], device='cuda:0')"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move tensor to GPU (if available)\n",
    "tensor_on_gpu = tensor.to(device)\n",
    "tensor_on_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4cab21ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[151]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# If tensor is on GPU, can't transform it to NumPy\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtensor_on_gpu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "# If tensor is on GPU, can't transform it to NumPy\n",
    "tensor_on_gpu.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a38669aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To fix the GPU tensor with NumPy issue, we can first set it to the CPU\n",
    "tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\n",
    "tensor_back_on_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b4c5be70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 3], device='cuda:0'), 'cpu')"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_on_gpu,tensor_back_on_cpu.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c988b104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-13278.1016)\n",
      "Time taken (ms): 215.99609375\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.randn(1000, 1000, 1000)\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "start.record()\n",
    "print(tensor.sum())\n",
    "end.record()\n",
    "torch.cuda.synchronize()\n",
    "print(f\"Time taken (ms): {start.elapsed_time(end)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a4bc214f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-13278.1045, device='cuda:0')\n",
      "Time taken (ms): 6.910431861877441\n"
     ]
    }
   ],
   "source": [
    "tensor_gpu = tensor.to(device)\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "start.record()\n",
    "print(tensor_gpu.sum())\n",
    "end.record()\n",
    "torch.cuda.synchronize()\n",
    "print(f\"Time taken (ms): {start.elapsed_time(end)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f2a12c",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "\n",
    "很明显gpu的运算速度更快"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
