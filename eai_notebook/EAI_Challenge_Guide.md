# 🧭 NeurIPS 2025 EAI Challenge：

## 一、先明确：比赛不是造 Agent，而是比「推理质量」

> 这是一个**离线推理赛（offline inference challenge）**，比的是「模型对具身任务的理解与规划能力」，而不是谁能做一个能动的 Agent」。

比赛由四个模块组成，每个模块测试不同的推理能力：

| 模块 | 本质在测什么 | 类比 |
|------|----------------|------|
| **Goal Interpretation (GI)** | 模型是否能把自然语言目标正确转成符号逻辑（LTL / structured goal） | 语言理解 + 逻辑翻译 |
| **Subgoal Decomposition (SD)** | 能否把复杂任务拆解成合理步骤 | 分层规划能力 |
| **Action Sequencing (AS)** | 动作序列是否可执行、顺序合理 | 计划生成 |
| **Transition Modeling (TM)** | 模型是否能预测状态转移（执行后世界变化） | 因果推理 / 物理常识 |

👉 本质考的是 **“LLM 的具身推理链”**：理解 → 分解 → 规划 → 推理。

---

## 二、参赛基本路线

### **Step 1. 熟悉输入与输出**
- 从 GitHub 官方仓库（`embodied-agent-interface`）下载 starter kit。  
- 每个任务都有 `input.json`（给你的指令）和 `output.json`（你要提交的答案）。  
- 官方提供的 `eval.py` 会自动评测这些输出。  

✅ **目标**：能手动看懂输入，并知道正确输出是什么样。

---

### **Step 2. 选择建模方式**

#### 🩵 路线 A：Prompt 工程 + 大模型（适合时间紧）
- 使用强大的通用模型（如 GPT-4o、Claude 3、DeepSeek-Chat、Qwen 2.5-72B）。  
- 为四个模块分别写 Prompt 模板，输出严格结构化。  

优点：上手快、效果强。  
缺点：成本高、可控性低。

#### 🩶 路线 B：轻量 Finetune（适合追求高名次）
- 收集官方数据集（GI、SD、AS、TM）。  
- 用开源模型（T5、Mistral-7B、Llama-3-8B、Qwem-2.5-7B）分别微调四个模型。  
- 强化 **AS（动作序列）** 模块的训练。

优点：可持续提升。  
缺点：前期工程量稍大。

---

### **Step 3. 搭建评测与提交流程**
1. 写一个脚本批量生成所有模块结果。  
2. 校验 JSON 格式。  
3. 打包为 `submission.zip` 上传 EvalAI。  

✅ **目标**：1 小时内能自动完成从输入到提交。

---

## 三、拿高分的关键思路（核心策略）

### 1️⃣ 结构化生成
- 输出必须是严格 JSON / LTL，格式错误会直接 0 分。  
- Prompt 中明确要求：
  ```
  Please output strictly in valid JSON following this schema...
  ```
- 用正则或脚本过滤非法输出。

---

### 2️⃣ 模块分治
- 不要一个模型做四个任务。  
- GI/SD 偏语言理解，可用 CoT + 模板。  
- AS 偏规划，要加入前置条件约束。  
- TM 偏推理，建议用状态差分描述。

---

### 3️⃣ 错误自检
- 官方评测会统计错误类型（幻觉、逻辑错、执行失败）。  
- 你可以让模型先生成结果，再让它自检：
  ```
  Check if this plan violates any precondition or repeats redundant actions.
  ```
- 这种 “self-verification” 机制能明显提高得分。

---

### 4️⃣ 多样性 + 一致性投票
- 对同一条输入，生成三次 → 取一致性最高的版本提交。  
- 比单次 greedy 输出更稳。

---

### 5️⃣ 注重执行成功率
- 评测最终看“计划是否能执行成功”。  
- 即便逻辑对，如果顺序错或缺条件，也算失败。  
- 所以 **Action Sequencing** 的优化最关键。

---

## 四、冲击高名次路线（前 10% 策略）

| 阶段 | 目标 | 核心动作 |
|------|------|----------|
| **准备期** | 跑通官方示例 | 理解评测逻辑与格式 |
| **建模期** | 各模块独立建模 | GI/SD 用模板，AS 微调，TM few-shot |
| **验证期** | 离线自测 | 用 `eval.py` 计算 F1 / success rate |
| **冲榜期** | 提交调优 | 调 prompt、temperature、beam size |
| **收尾期** | 稳定提交 | 保证格式正确与确定性输出 |

> 如果时间不够，只优化 **AS + SD** 两个模块，也能拿不错名次。

---

## 五、长期视角（想拿前三）

可以进一步做两件高阶策略：

1. **多模态辅助（可选）**  
   - 把 VirtualHome 的图像描述或对象信息加入 prompt，增强语义理解。  
2. **链式训练（Chain Training）**  
   - 用 SD 的输出训练 AS，形成轻量级端到端 pipeline。

---

## ✅ 总结一句话

> 这场比赛比的是 **逻辑推理与结构化规划的准确性**。  
> 高分来自：  
> **模块分治 + 结构化输出 + 自检机制 + 动作序列可靠性**。
